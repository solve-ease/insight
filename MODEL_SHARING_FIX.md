# GPU Memory Fix - Model Sharing

## Problem
Loading TWO separate CLIP models (one for images, one for videos) was using ~3.6GB of your 4GB GPU, leaving no memory for actual processing.

## Solution
**Share the same CLIP model** between image and video processors.

## Changes Made

### 1. app.py
- Now loads only ONE CLIP model
- Shares it between image and video processors
- Saves ~1.8GB of GPU memory

### 2. video_to_embedding.py
- Added `shared_model` parameter to `VideoEmbeddingProcessor`
- Can accept an existing ImageEmbeddingProcessor to reuse its model

### 3. video_embeddings/embedding.py
- Added `shared_clip_model` and `shared_clip_processor` parameters
- Uses shared model if provided, otherwise loads new one

## Memory Usage

**Before (FAILED):**
```
Image Model:     ~1.8GB
Video Model:     ~1.8GB
Total:           ~3.6GB / 4GB
Available:       ~0.4GB (NOT ENOUGH!)
```

**After (SUCCESS):**
```
Shared Model:    ~1.8GB
Total:           ~1.8GB / 4GB
Available:       ~2.2GB (PLENTY!)
```

## Usage

### Streamlit App (Automatic)
Just run the app - model sharing is automatic:
```bash
streamlit run app.py
```

### CLI (Standalone)
For standalone video processing, the model is loaded fresh:
```bash
python3 video_to_embedding.py
# Uses ~1.8GB - plenty of room!
```

## Testing

Stop the Streamlit app and restart:
```bash
# Ctrl+C to stop
streamlit run app.py
```

You should now see:
```
Loading CLIP model (shared for images & videos)...
âœ“ Model loaded successfully! (Shared between image & video search)
ðŸ’¡ Using shared CLIP model for both images and videos to optimize GPU memory
```

Then try indexing videos - it should work without OOM errors!

## Performance Impact
- **Memory**: 50% reduction (~3.6GB â†’ ~1.8GB)
- **Speed**: NO impact - same model performance
- **Quality**: NO impact - same embeddings
- **Benefit**: Can now actually process videos on 4GB GPU!

---

**Status**: Ready to test! The CUDA OOM should now be completely resolved.
